{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_style('whitegrid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv('AmesHousing.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "df.info(verbose=False) prints a concise summary of a DataFrame, omitting the detailed information for each column. This is especially useful for large DataFrames with many columns, where the full output of df.info() can be overwhelming and hard to read. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info(verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "Min (Minimum): The smallest value in a dataset. It can help identify the lower bound of your data and can sometimes reveal data entry errors (e.g., a negative value for age).\n",
    "Mean (Average): The sum of all values divided by the number of values in the dataset. It is a measure of the central tendency of the data but is sensitive to outliers, which can skew the result.\n",
    "Max (Maximum): The largest value in a dataset. It establishes the upper boundary of your data and, like the minimum, can help identify outliers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['SalePrice'].describe() # Check the mean, min, and max of the house prices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,6))\n",
    "sns.histplot(df['SalePrice'], kde=True, bins=50)\n",
    "plt.title('Distribution of House Sale Prices')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "### 2. Cleaning the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Task 3: Quantify Missingness \n",
    "Calculate and Sort Missing Percentages:\n",
    "'''\n",
    "total_missing = df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "df.isnull().sum() / df.isnull().count(): Dividing the count of missing values by the total count of values gives the proportion of missing data for each column. The result is a pandas Series where the index is the column name and the value is the percentage (as a decimal) of missing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "percent_missing=(df.isnull().sum()/df.isnull().count()).sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_data=pd.concat([total_missing, percent_missing], axis=1, keys=['Total', 'Percent'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_data.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20",
   "metadata": {},
   "source": [
    "#### Missing Data Imputation Strategy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21",
   "metadata": {},
   "source": [
    "##### 1. Drop columns with too much missing data \n",
    "First, you will remove columns that contain a high percentage of missing values to avoid introducing too much bias during imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_drop = percent_missing[percent_missing > 0.50].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaned=df.drop(columns=columns_to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaned"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25",
   "metadata": {},
   "source": [
    "##### 2. Fill categorical columns with 'None' \n",
    "For categorical features where the missing value represents the absence of a feature (e.g., no garage), you can fill NaN values with the string 'None'. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_fill_none = ['GarageType', 'GarageFinish', 'GarageQual', 'GarageCond', \n",
    "                     'BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', \n",
    "                     'BsmtFinType2', 'MasVnrType']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in cols_to_fill_none:\n",
    "    if col in df.columns:\n",
    "        df[col].fillna('None', inplace=True)\n",
    "\n",
    "print(\"Filled categorical columns with 'None'.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28",
   "metadata": {},
   "source": [
    "##### 3. Fill numerical columns with median or zero \n",
    "For numerical columns, the choice of imputation depends on the data's distribution. The median is robust against outliers, making it a good choice for skewed data like LotFrontage. Filling with 0 is appropriate when the absence of a value truly means a zero value (e.g., in some cases for GarageYrBlt or MasVnrArea). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Fill LotFrontage with the median\n",
    "if 'LotFrontage' in df.columns:\n",
    "    df['LotFrontage'].fillna(df['LotFrontage'].median(), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill GarageYrBlt with 0, as houses without a garage might have a missing year\n",
    "if 'GarageYrBlt' in df.columns:\n",
    "    df['GarageYrBlt'].fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example for other numerical columns that can be filled with 0\n",
    "if 'MasVnrArea' in df.columns:\n",
    "    df['MasVnrArea'].fillna(0, inplace=True)\n",
    "\n",
    "print(\"Filled numerical columns with median or 0.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32",
   "metadata": {},
   "source": [
    "##### 4. Fill categorical columns with mode\n",
    "For categorical columns with only a few missing values that aren't meaningfully absent, use the mode (most frequent value). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill 'Electrical' column with its mode\n",
    "if 'Electrical' in df.columns:\n",
    "    df['Electrical'].fillna(df['Electrical'].mode()[0], inplace=True)\n",
    "\n",
    "print(\"Filled categorical column 'Electrical' with mode.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34",
   "metadata": {},
   "source": [
    "##### 5. Drop High-Null Columns\n",
    "\n",
    "We are dropping the columns that were nearly all null, as these are not useful for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify columns to drop (based on a typical EDA where missing percentage is > 50-80%)\n",
    "cols_to_drop = ['Alley', 'Fence']\n",
    "df.drop(cols_to_drop, axis=1, inplace=True)\n",
    "\n",
    "# Note: We are keeping the Garage and Basement columns for now, as missingness here means 'None' (absence of the feature).\n",
    "print(f\"Dropped columns: {cols_to_drop}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info(verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37",
   "metadata": {},
   "source": [
    "### Phase 3: Bivariate Analysis (Feature vs. SalePrice)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38",
   "metadata": {},
   "source": [
    "##### Task 5: Numerical Correlation Analysis ðŸ”¢\n",
    "\n",
    "Since numerical data is easy to correlate, let's see the linear relationship between all numerical features and SalePrice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate correlation matrix\n",
    "corrmat = df.corr(numeric_only=True)\n",
    "\n",
    "# Get the correlation of all features with 'SalePrice' and sort them\n",
    "corr_with_saleprice = corrmat['SalePrice'].sort_values(ascending=False)\n",
    "\n",
    "print(corr_with_saleprice.head(10)) # Top 10 positive correlations\n",
    "print(corr_with_saleprice.tail(10)) # Bottom 10 (negative) correlations"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "prac_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
